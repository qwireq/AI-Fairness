{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Script_AI-Fairness.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdAU8m7Bzb8i",
        "colab_type": "code",
        "outputId": "b417a252-31c9-4373-dbe4-9c22570757f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4WKwevUzmYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "import os,sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn import feature_extraction\n",
        "from sklearn import preprocessing\n",
        "from random import seed, shuffle\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRYY7Sx0GXzh",
        "colab_type": "text"
      },
      "source": [
        "Dataset preparation code was taken from https://github.com/mbilalzafar/fair-classification/tree/master/disparate_mistreatment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyDaHpjh_IiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_into_train_test(x_all, y_all, x_control_all, train_fold_size):\n",
        "\n",
        "    split_point = int(round(float(x_all.shape[0]) * train_fold_size))\n",
        "    x_all_train = x_all[:split_point]\n",
        "    x_all_test = x_all[split_point:]\n",
        "    y_all_train = y_all[:split_point]\n",
        "    y_all_test = y_all[split_point:]\n",
        "    x_control_all_train = {}\n",
        "    x_control_all_test = {}\n",
        "    for k in x_control_all.keys():\n",
        "        x_control_all_train[k] = x_control_all[k][:split_point]\n",
        "        x_control_all_test[k] = x_control_all[k][split_point:]\n",
        "\n",
        "    return x_all_train, y_all_train, x_control_all_train, x_all_test, y_all_test, x_control_all_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IrqmgI8_PZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_intercept(x):\n",
        "\n",
        "    \"\"\" Add intercept to the data before linear classification \"\"\"\n",
        "    m,n = x.shape\n",
        "    intercept = np.ones(m).reshape(m, 1) # the constant b\n",
        "    return np.concatenate((intercept, x), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxUcPX2xdkDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sys.path.insert(0, '../../fair_classification/') # the code for fair classification is in this directory\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\"\"\"\n",
        "    The adult dataset can be obtained from: https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
        "    The code will look for the data file in the present directory, if it is not found, it will download them from GitHub.\n",
        "\"\"\"\n",
        "\n",
        "def check_data_file(fname):\n",
        "    files = os.listdir(\"drive/My Drive/KAIST/Lab/COMPAS/\") # get the current directory listing\n",
        "    print (\"Looking for file '%s' in the current directory...\" % fname)\n",
        "\n",
        "    if fname not in files:\n",
        "    \n",
        "      print(\"'%s' not found! Downloading from GitHub...\" % fname)\n",
        "      addr = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
        "      response = urllib.request.urlopen(addr)\n",
        "\n",
        "      data = response.read()\n",
        "      print(data)\n",
        "      fileOut = open(\"drive/My Drive/KAIST/Lab/COMPAS/\"+fname, \"w\")\n",
        "      fileOut.write(data.decode(\"utf-8\") )\n",
        "      fileOut.close()\n",
        "      print( \"'%s' download and saved locally..\" % fname)\n",
        "    else:\n",
        "      print( \"File found in current directory..\")\n",
        "    \n",
        "\n",
        "def load_compas_data():\n",
        "\n",
        "\n",
        "\tFEATURES_CLASSIFICATION = [\"age\",\"race\", \"sex\", \"juv_fel_count\", \"juv_misd_count\", \"juv_other_count\", \"priors_count\", \"c_charge_degree\"] #features to be used for classification\n",
        "\tCONT_VARIABLES = [\"priors_count\"] # continuous features, will need to be handled separately from categorical features, categorical features will be encoded using one-hot\n",
        "\tCLASS_FEATURE = \"two_year_recid\" # the decision variable\n",
        "\tSENSITIVE_ATTRS = [\"race\"]\n",
        "\n",
        "\n",
        "\tCOMPAS_INPUT_FILE = \"compas-scores-two-years3.csv\"\n",
        "\tcheck_data_file(COMPAS_INPUT_FILE)\n",
        "\n",
        "\t# load the data and get some stats\n",
        "\tdf = pd.read_csv(\"drive/My Drive/KAIST/Lab/COMPAS/\" + COMPAS_INPUT_FILE)\n",
        "\tdf = df.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\n",
        "\t\n",
        "\t# convert to np array\n",
        "\tdata = df.to_dict('list')\n",
        "\tfor k in data.keys():\n",
        "\t\tdata[k] = np.array(data[k])\n",
        "\n",
        "\n",
        "\t\"\"\" Filtering the data \"\"\"\n",
        "\n",
        "\t# These filters are the same as propublica (refer to https://github.com/propublica/compas-analysis)\n",
        "\t# If the charge date of a defendants Compas scored crime was not within 30 days from when the person was arrested, we assume that because of data quality reasons, that we do not have the right offense. \n",
        "\tidx = np.logical_and(data[\"days_b_screening_arrest\"]<=30, data[\"days_b_screening_arrest\"]>=-30)\n",
        "\n",
        "\n",
        "\t# We coded the recidivist flag -- is_recid -- to be -1 if we could not find a compas case at all.\n",
        "\tidx = np.logical_and(idx, data[\"is_recid\"] != -1)\n",
        "\n",
        "\t# In a similar vein, ordinary traffic offenses -- those with a c_charge_degree of 'O' -- will not result in Jail time are removed (only two of them).\n",
        "\tidx = np.logical_and(idx, data[\"c_charge_degree\"] != \"O\") # F: felony, M: misconduct\n",
        "\n",
        "\t# We filtered the underlying data from Broward county to include only those rows representing people who had either recidivated in two years, or had at least two years outside of a correctional facility.\n",
        "\tidx = np.logical_and(idx, data[\"score_text\"] != \"NA\")\n",
        "\n",
        "\t# we will only consider blacks and whites for this analysis\n",
        "\tidx = np.logical_and(idx, np.logical_or(data[\"race\"] == \"African-American\", data[\"race\"] == \"Caucasian\"))\n",
        "\n",
        "\t# select the examples that satisfy this criteria\n",
        "\tfor k in data.keys():\n",
        "\t\tdata[k] = data[k][idx]\n",
        "\n",
        "\n",
        "\n",
        "\t\"\"\" Feature normalization and one hot encoding \"\"\"\n",
        "\n",
        "\t# convert class label 0 to -1\n",
        "\ty = data[CLASS_FEATURE]\n",
        "\ty[y==0] = -1\n",
        "\n",
        "\t\n",
        "\t\n",
        "\tprint( \"\\nNumber of people recidivating within two years\")\n",
        "\tprint( pd.Series(y).value_counts())\n",
        "\tprint( \"\\n\")\n",
        "\n",
        "\n",
        "\tX = np.array([]).reshape(len(y), 0) # empty array with num rows same as num examples, will hstack the features to it\n",
        "\tx_control = defaultdict(list)\n",
        "\n",
        "\tfeature_names = []\n",
        "\tfor attr in FEATURES_CLASSIFICATION:\n",
        "\t\tvals = data[attr]\n",
        "\t\tif attr in CONT_VARIABLES:\n",
        "\t\t\tvals = [float(v) for v in vals]\n",
        "\t\t\tvals = preprocessing.scale(vals) # 0 mean and 1 variance  \n",
        "\t\t\tvals = np.reshape(vals, (len(y), -1)) # convert from 1-d arr to a 2-d arr with one col\n",
        "\n",
        "\t\telse: # for binary categorical variables, the label binarizer uses just one var instead of two\n",
        "\t\t\tlb = preprocessing.LabelBinarizer()\n",
        "\t\t\tlb.fit(vals)\n",
        "\t\t\tvals = lb.transform(vals)\n",
        "\n",
        "\t\t# add to sensitive features dict\n",
        "\t\tif attr in SENSITIVE_ATTRS:\n",
        "\t\t\tx_control[attr] = vals\n",
        "\n",
        "\n",
        "\t\t# add to learnable features\n",
        "\t\tX = np.hstack((X, vals))\n",
        "\n",
        "\t\tif attr in CONT_VARIABLES: # continuous feature, just append the name\n",
        "\t\t\tfeature_names.append(attr)\n",
        "\t\telse: # categorical features\n",
        "\t\t\tif vals.shape[1] == 1: # binary features that passed through lib binarizer\n",
        "\t\t\t\tfeature_names.append(attr)\n",
        "\t\t\telse:\n",
        "\t\t\t\tfor k in lb.classes_: # non-binary categorical features, need to add the names for each cat\n",
        "\t\t\t\t\tfeature_names.append(attr + \"_\" + str(k))\n",
        "\n",
        "\n",
        "\t# convert the sensitive feature to 1-d array\n",
        "\tx_control = dict(x_control)\n",
        "\tfor k in x_control.keys():\n",
        "\t\tassert(x_control[k].shape[1] == 1) # make sure that the sensitive feature is binary after one hot encoding\n",
        "\t\tx_control[k] = np.array(x_control[k]).flatten()\n",
        "\n",
        "\t# sys.exit(1)\n",
        "\n",
        "\t\"\"\"permute the date randomly\"\"\"\n",
        "\tperm = list(range(0,X.shape[0]))\n",
        "\tshuffle(perm)\n",
        "\tX = X[perm]\n",
        "\ty = y[perm]\n",
        "\tfor k in x_control.keys():\n",
        "\t\tx_control[k] = x_control[k][perm]\n",
        "\n",
        "\n",
        "\tX = add_intercept(X)\n",
        "\n",
        "\tfeature_names = [\"intercept\"] + feature_names\n",
        "\tassert(len(feature_names) == X.shape[1])\n",
        "\tprint( \"Features we will be using for classification are:\", feature_names, \"\\n\")\n",
        "\n",
        "\n",
        "\treturn X, y, x_control"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgrHs-saQphh",
        "colab_type": "code",
        "outputId": "aa1473f1-97ba-4902-ec1b-24e7a746708a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # activation function ReLU\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = nn.functional.softmax(x)\n",
        "        #x = F.one_hot(x, num_classes=2)\n",
        "        return x\n",
        "\n",
        "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "model = Net(input_size=94, hidden_size=100, num_classes=2).to(device)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=94, out_features=100, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63H-xRn4Wirs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=1)\n",
        "sgd = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1)\n",
        "optimizer = adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Cc-6_1ZAR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global my_counter\n",
        "my_counter = 0\n",
        "def helper_loss(out, trg, alpha):\n",
        "    n = out.shape[0]\n",
        "    benefit = 0\n",
        "    utility = 0\n",
        "    mean_benefit = 0\n",
        "    tens_benefit = torch.tensor(0.0)\n",
        "    for it in range(0, n):\n",
        "        if  trg[it][0].item() > trg[it][1].item():\n",
        "            c = 0.25\n",
        "            d = 1.25\n",
        "        else:\n",
        "            c = 0.5\n",
        "            d = 0.5\n",
        "        if  out[it][0].item() > out[it][1].item():\n",
        "            benefit = (-c*out[it][0].item() + d)**alpha\n",
        "            tens_benefit.add_((-c*out[it][0]+d)**alpha)\n",
        "        else:\n",
        "            benefit = (c**out[it][1].item() + d)**alpha\n",
        "            tens_benefit.add_((c*out[it][1]+d)**alpha)\n",
        "        \n",
        "        mean_benefit += benefit\n",
        "        utility += benefit\n",
        "    #tens_benefit = torch.sum(tens_benefit**alpha)\n",
        "    #tens_utility = torch.sum(tens_benefit)\n",
        "    tens_utility = tens_benefit\n",
        "    return utility, n, tens_utility\n",
        "\n",
        "def my_torch_loss(alpha, tau, lgrng_mult = 100):\n",
        "    def utility_loss(output, target):        \n",
        "        \n",
        "        utility, n, tens_utility = helper_loss(output, target, alpha)\n",
        "        global my_counter\n",
        "        my_counter += 1\n",
        "        sign = 1\n",
        "        \n",
        "        sq_loss = torch.sum((output - target)**2)\n",
        "        if tens_utility - tau*n > 0:\n",
        "            sign = -0.1\n",
        "        if my_counter%1 == 0:\n",
        "            print(\"\\n\\n\\t\\tUTILITY: \", tens_utility, \"\\tTAU*n: \", tau*n, \" Regularizer loss: \", -sign*lgrng_mult*(tens_utility -tau*n), \"SQ loss: \",sq_loss)\n",
        "        \n",
        "        #loss = sq_loss - torch.tensor(lgrng_mult*sign*(tens_utility - tau*n), requires_grad=True)\n",
        "        #loss = sq_loss - lgrng_mult*sign*(tens_utility.add_(-tau*n))\n",
        "        loss = sq_loss - sign*lgrng_mult*(tens_utility - tau*n)\n",
        "        return loss\n",
        "    \n",
        "    return utility_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMzXXFsoj-Ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = my_torch_loss(0.5, 0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L8uFaRN-Q0VH",
        "colab": {}
      },
      "source": [
        "#WITH BATCH SIZE = dataset size.\n",
        "from torch.utils import data\n",
        "def train_test_classifier(x_train, y_true, x_test, y_test):\n",
        "    x = torch.from_numpy(x_train)\n",
        "    y = torch.from_numpy(y_true)\n",
        "    print(\"TRAIN ON:\\t\", device, '\\n')\n",
        "\n",
        "    my_dataset = data.TensorDataset(x,y) # create your datset\n",
        "    my_dataloader = data.DataLoader(my_dataset, batch_size=4222,\n",
        "                                          shuffle=True, num_workers=2) # create your dataloader\n",
        "    print(\"Training DATASET SIZE: \", len(my_dataset), \"\\tNUM OF BATCHES: \", len(my_dataset)/4222)\n",
        "    for t in range(50):\n",
        "        # Forward pass: compute predicted y by passing x to the model.\n",
        "        running_loss = 0.0\n",
        "        for i, batch in enumerate(my_dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = batch\n",
        "            x_cur = inputs.to(device)\n",
        "            y_cur = labels.to(device)\n",
        "\n",
        "            y_pred = model(x_cur.float())\n",
        "            #print(\"Prediction:\\t\", y_pred, \"\\n\\n\\n\")\n",
        "            #_, y_pr_ind = torch.max(y_pred, 1)\n",
        "            #_, y_cur_ind = torch.max(y_cur, 1)\n",
        "            # Compute and print loss.\n",
        "            loss = loss_fn(y_pred, y_cur.long())\n",
        "            running_loss += loss.item()\n",
        "            #if (i+1) % 4222 == 0:\n",
        "               #print(\"Epoch: \", t+1, \":\", (i+1)*4222, \"\\tLoss: \", running_loss)\n",
        "                #running_loss = 0\n",
        "            if t % 50 == 0:\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['lr'] = g['lr'] / 10\n",
        "  \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(\"Epoch: \", t+1, \"\\tLoss: \", running_loss)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    x_t = torch.from_numpy(x_train).to(device)\n",
        "    y_t = torch.from_numpy(y_true).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x_t.float())\n",
        "        #print(outputs)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        _, tr = torch.max(y_t, 1)\n",
        "        total += outputs.shape[0]\n",
        "        print(pred)\n",
        "        print(tr)\n",
        "        correct += (pred == tr).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the train set: %f total: %d' % (\n",
        "        correct/total, total))\n",
        "    \n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    x_t = torch.from_numpy(x_test).to(device)\n",
        "    y_t = torch.from_numpy(y_test).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(x_t.float())\n",
        "        #print(outputs)\n",
        "        _, pred = torch.max(outputs, 1)\n",
        "        _, tr = torch.max(y_t, 1)\n",
        "        total += outputs.shape[0]\n",
        "        print(pred)\n",
        "        print(tr)\n",
        "        correct += (pred == tr).sum().item()\n",
        "\n",
        "    print('\\n\\nAccuracy of the network on test set: %f total: %d' % (\n",
        "        correct/total, total))\n",
        "    \n",
        "    score = correct\n",
        "\n",
        "    return score, \"correct\" #[score, model.metrics_names]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "aa1439ba-738b-49ce-a2bf-342c6cf70943",
        "id": "ZYRd638XBGt1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "def test_compas_data():\n",
        "  ''' Get the data \t'''\n",
        "  data_type = 1\n",
        "  X, y, x_control = load_compas_data()\n",
        "  sensitive_attrs = x_control.keys()\n",
        "\n",
        "\n",
        "  ''' Split the data into train and test \t'''\n",
        "  train_fold_size = 0.8\n",
        "  \n",
        "  x_train, y_train, x_control_train, x_test, y_test, x_control_test = split_into_train_test(X, y, x_control, train_fold_size)\n",
        "  #print(y_train[:20])\n",
        "  lb = LabelEncoder()\n",
        "  #y_train = one_hot_encode(lb.fit_transform(y_train))\n",
        "  #y_test = np_utils.to_categorical(one_hot_encode(y_test))\n",
        "  y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "  y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
        "  print(x_train.shape, y_test.shape)\n",
        "  print(\"classes: \", lb.classes_, \" turned into \", [0, 1])\n",
        "  score1 = train_test_classifier(x_train, y_train, x_test, y_test)\n",
        "  \n",
        "  print(\"\\n\\nTEST ACCURACY:\", score1)\n",
        "  return y_train, score1\n",
        "\n",
        "\n",
        "def main():\n",
        "  return test_compas_data()\n",
        "\n",
        "y_train = main()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking for file 'compas-scores-two-years3.csv' in the current directory...\n",
            "File found in current directory..\n",
            "\n",
            "Number of people recidivating within two years\n",
            "-1    2795\n",
            " 1    2483\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "Features we will be using for classification are: ['intercept', 'age_18', 'age_19', 'age_20', 'age_21', 'age_22', 'age_23', 'age_24', 'age_25', 'age_26', 'age_27', 'age_28', 'age_29', 'age_30', 'age_31', 'age_32', 'age_33', 'age_34', 'age_35', 'age_36', 'age_37', 'age_38', 'age_39', 'age_40', 'age_41', 'age_42', 'age_43', 'age_44', 'age_45', 'age_46', 'age_47', 'age_48', 'age_49', 'age_50', 'age_51', 'age_52', 'age_53', 'age_54', 'age_55', 'age_56', 'age_57', 'age_58', 'age_59', 'age_60', 'age_61', 'age_62', 'age_63', 'age_64', 'age_65', 'age_66', 'age_67', 'age_68', 'age_69', 'age_70', 'age_71', 'age_72', 'age_73', 'age_74', 'age_75', 'age_77', 'age_78', 'age_79', 'age_80', 'race', 'sex', 'juv_fel_count_0', 'juv_fel_count_1', 'juv_fel_count_2', 'juv_fel_count_3', 'juv_fel_count_4', 'juv_fel_count_5', 'juv_fel_count_6', 'juv_fel_count_8', 'juv_fel_count_10', 'juv_misd_count_0', 'juv_misd_count_1', 'juv_misd_count_2', 'juv_misd_count_3', 'juv_misd_count_4', 'juv_misd_count_5', 'juv_misd_count_6', 'juv_misd_count_8', 'juv_misd_count_12', 'juv_misd_count_13', 'juv_other_count_0', 'juv_other_count_1', 'juv_other_count_2', 'juv_other_count_3', 'juv_other_count_4', 'juv_other_count_5', 'juv_other_count_6', 'juv_other_count_7', 'priors_count', 'c_charge_degree'] \n",
            "\n",
            "(4222, 94) (1056, 2)\n",
            "classes:  [-1  1]  turned into  [0, 1]\n",
            "TRAIN ON:\t cpu \n",
            "\n",
            "Training DATASET SIZE:  4222 \tNUM OF BATCHES:  1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3672.5283, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(33837.1562, grad_fn=<MulBackward0>) SQ loss:  tensor(2083.3782, grad_fn=<SumBackward0>)\n",
            "Epoch:  1 \tLoss:  35920.53515625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4421.4761, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4105.7617, grad_fn=<MulBackward0>) SQ loss:  tensor(2194.2744, grad_fn=<SumBackward0>)\n",
            "Epoch:  2 \tLoss:  6300.0361328125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4466.0972, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4551.9727, grad_fn=<MulBackward0>) SQ loss:  tensor(2328.4280, grad_fn=<SumBackward0>)\n",
            "Epoch:  3 \tLoss:  6880.400390625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4488.3491, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4774.4922, grad_fn=<MulBackward0>) SQ loss:  tensor(2412.3767, grad_fn=<SumBackward0>)\n",
            "Epoch:  4 \tLoss:  7186.869140625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4497.0156, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4861.1572, grad_fn=<MulBackward0>) SQ loss:  tensor(2444.1650, grad_fn=<SumBackward0>)\n",
            "Epoch:  5 \tLoss:  7305.322265625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4497.3511, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4864.5117, grad_fn=<MulBackward0>) SQ loss:  tensor(2438.3293, grad_fn=<SumBackward0>)\n",
            "Epoch:  6 \tLoss:  7302.8408203125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4492.4312, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4815.3125, grad_fn=<MulBackward0>) SQ loss:  tensor(2408.6206, grad_fn=<SumBackward0>)\n",
            "Epoch:  7 \tLoss:  7223.93310546875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4484.2217, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4733.2178, grad_fn=<MulBackward0>) SQ loss:  tensor(2365.8623, grad_fn=<SumBackward0>)\n",
            "Epoch:  8 \tLoss:  7099.080078125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4473.8843, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4629.8438, grad_fn=<MulBackward0>) SQ loss:  tensor(2317.6724, grad_fn=<SumBackward0>)\n",
            "Epoch:  9 \tLoss:  6947.51611328125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4462.2661, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4513.6621, grad_fn=<MulBackward0>) SQ loss:  tensor(2269.1997, grad_fn=<SumBackward0>)\n",
            "Epoch:  10 \tLoss:  6782.86181640625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4449.9092, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4390.0928, grad_fn=<MulBackward0>) SQ loss:  tensor(2223.2651, grad_fn=<SumBackward0>)\n",
            "Epoch:  11 \tLoss:  6613.35791015625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4437.1489, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4262.4902, grad_fn=<MulBackward0>) SQ loss:  tensor(2181.7378, grad_fn=<SumBackward0>)\n",
            "Epoch:  12 \tLoss:  6444.22802734375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4424.2817, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4133.8184, grad_fn=<MulBackward0>) SQ loss:  tensor(2145.7056, grad_fn=<SumBackward0>)\n",
            "Epoch:  13 \tLoss:  6279.52392578125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4411.5034, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(4006.0352, grad_fn=<MulBackward0>) SQ loss:  tensor(2115.4751, grad_fn=<SumBackward0>)\n",
            "Epoch:  14 \tLoss:  6121.51025390625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4398.8589, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3879.5898, grad_fn=<MulBackward0>) SQ loss:  tensor(2090.7417, grad_fn=<SumBackward0>)\n",
            "Epoch:  15 \tLoss:  5970.33154296875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4386.1665, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3752.6660, grad_fn=<MulBackward0>) SQ loss:  tensor(2070.5361, grad_fn=<SumBackward0>)\n",
            "Epoch:  16 \tLoss:  5823.2021484375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4360.5410, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3496.4111, grad_fn=<MulBackward0>) SQ loss:  tensor(2054.6536, grad_fn=<SumBackward0>)\n",
            "Epoch:  17 \tLoss:  5551.064453125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4089.6807, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(787.8076, grad_fn=<MulBackward0>) SQ loss:  tensor(2043.1411, grad_fn=<SumBackward0>)\n",
            "Epoch:  18 \tLoss:  2830.94873046875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3744.3196, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(26658.0312, grad_fn=<MulBackward0>) SQ loss:  tensor(2035.8705, grad_fn=<SumBackward0>)\n",
            "Epoch:  19 \tLoss:  28693.90234375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3809.3855, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(20151.4414, grad_fn=<MulBackward0>) SQ loss:  tensor(2038.1749, grad_fn=<SumBackward0>)\n",
            "Epoch:  20 \tLoss:  22189.6171875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4078.6794, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(677.7954, grad_fn=<MulBackward0>) SQ loss:  tensor(2041.5278, grad_fn=<SumBackward0>)\n",
            "Epoch:  21 \tLoss:  2719.3232421875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4272.9766, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(2620.7666, grad_fn=<MulBackward0>) SQ loss:  tensor(2045.6969, grad_fn=<SumBackward0>)\n",
            "Epoch:  22 \tLoss:  4666.46337890625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4357.9111, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3470.1123, grad_fn=<MulBackward0>) SQ loss:  tensor(2050.0205, grad_fn=<SumBackward0>)\n",
            "Epoch:  23 \tLoss:  5520.1328125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4376.6582, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3657.5830, grad_fn=<MulBackward0>) SQ loss:  tensor(2053.9116, grad_fn=<SumBackward0>)\n",
            "Epoch:  24 \tLoss:  5711.49462890625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4382.2378, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3713.3789, grad_fn=<MulBackward0>) SQ loss:  tensor(2056.9065, grad_fn=<SumBackward0>)\n",
            "Epoch:  25 \tLoss:  5770.28515625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4384.7710, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3738.7109, grad_fn=<MulBackward0>) SQ loss:  tensor(2058.9111, grad_fn=<SumBackward0>)\n",
            "Epoch:  26 \tLoss:  5797.6220703125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4386.2119, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3753.1201, grad_fn=<MulBackward0>) SQ loss:  tensor(2059.9360, grad_fn=<SumBackward0>)\n",
            "Epoch:  27 \tLoss:  5813.05615234375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4386.8306, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3759.3066, grad_fn=<MulBackward0>) SQ loss:  tensor(2060.1196, grad_fn=<SumBackward0>)\n",
            "Epoch:  28 \tLoss:  5819.42626953125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4386.7275, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3758.2764, grad_fn=<MulBackward0>) SQ loss:  tensor(2059.6006, grad_fn=<SumBackward0>)\n",
            "Epoch:  29 \tLoss:  5817.876953125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4385.9971, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3750.9717, grad_fn=<MulBackward0>) SQ loss:  tensor(2058.5789, grad_fn=<SumBackward0>)\n",
            "Epoch:  30 \tLoss:  5809.55078125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4384.7573, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3738.5742, grad_fn=<MulBackward0>) SQ loss:  tensor(2057.2058, grad_fn=<SumBackward0>)\n",
            "Epoch:  31 \tLoss:  5795.7802734375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4383.0591, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3721.5918, grad_fn=<MulBackward0>) SQ loss:  tensor(2055.6389, grad_fn=<SumBackward0>)\n",
            "Epoch:  32 \tLoss:  5777.23046875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4380.9961, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3700.9619, grad_fn=<MulBackward0>) SQ loss:  tensor(2054.0234, grad_fn=<SumBackward0>)\n",
            "Epoch:  33 \tLoss:  5754.9853515625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4378.6426, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3677.4268, grad_fn=<MulBackward0>) SQ loss:  tensor(2052.5017, grad_fn=<SumBackward0>)\n",
            "Epoch:  34 \tLoss:  5729.9287109375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4376.0674, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3651.6748, grad_fn=<MulBackward0>) SQ loss:  tensor(2051.2190, grad_fn=<SumBackward0>)\n",
            "Epoch:  35 \tLoss:  5702.8935546875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4373.2192, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3623.1934, grad_fn=<MulBackward0>) SQ loss:  tensor(2050.2019, grad_fn=<SumBackward0>)\n",
            "Epoch:  36 \tLoss:  5673.3955078125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4369.6851, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3587.8516, grad_fn=<MulBackward0>) SQ loss:  tensor(2049.5071, grad_fn=<SumBackward0>)\n",
            "Epoch:  37 \tLoss:  5637.3583984375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4351.9692, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3410.6934, grad_fn=<MulBackward0>) SQ loss:  tensor(2049.1648, grad_fn=<SumBackward0>)\n",
            "Epoch:  38 \tLoss:  5459.8583984375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4303.7812, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(2928.8135, grad_fn=<MulBackward0>) SQ loss:  tensor(2049.1455, grad_fn=<SumBackward0>)\n",
            "Epoch:  39 \tLoss:  4977.958984375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4215.8965, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(2049.9658, grad_fn=<MulBackward0>) SQ loss:  tensor(2049.4595, grad_fn=<SumBackward0>)\n",
            "Epoch:  40 \tLoss:  4099.42529296875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4095.1575, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(842.5757, grad_fn=<MulBackward0>) SQ loss:  tensor(2050.0481, grad_fn=<SumBackward0>)\n",
            "Epoch:  41 \tLoss:  2892.623779296875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3985.8926, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(2500.7324, grad_fn=<MulBackward0>) SQ loss:  tensor(2050.8333, grad_fn=<SumBackward0>)\n",
            "Epoch:  42 \tLoss:  4551.5654296875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4018.6206, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(77.2070, grad_fn=<MulBackward0>) SQ loss:  tensor(2050.3735, grad_fn=<SumBackward0>)\n",
            "Epoch:  43 \tLoss:  2127.58056640625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4032.5264, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(216.2646, grad_fn=<MulBackward0>) SQ loss:  tensor(2049.9587, grad_fn=<SumBackward0>)\n",
            "Epoch:  44 \tLoss:  2266.223388671875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4040.3667, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(294.6680, grad_fn=<MulBackward0>) SQ loss:  tensor(2049.5867, grad_fn=<SumBackward0>)\n",
            "Epoch:  45 \tLoss:  2344.254638671875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4033.0286, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(221.2866, grad_fn=<MulBackward0>) SQ loss:  tensor(2049.2563, grad_fn=<SumBackward0>)\n",
            "Epoch:  46 \tLoss:  2270.54296875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4015.6409, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(47.4097, grad_fn=<MulBackward0>) SQ loss:  tensor(2048.9690, grad_fn=<SumBackward0>)\n",
            "Epoch:  47 \tLoss:  2096.378662109375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3971.1531, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(3974.6826, grad_fn=<MulBackward0>) SQ loss:  tensor(2048.7217, grad_fn=<SumBackward0>)\n",
            "Epoch:  48 \tLoss:  6023.404296875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4068.3420, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(574.4214, grad_fn=<MulBackward0>) SQ loss:  tensor(2047.1073, grad_fn=<SumBackward0>)\n",
            "Epoch:  49 \tLoss:  2621.52880859375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4170.0679, grad_fn=<AddBackward0>) \tTAU*n:  4010.8999999999996  Regularizer loss:  tensor(1591.6797, grad_fn=<MulBackward0>) SQ loss:  tensor(2045.7700, grad_fn=<SumBackward0>)\n",
            "Epoch:  50 \tLoss:  3637.44970703125\n",
            "tensor([1, 1, 1,  ..., 1, 1, 1])\n",
            "tensor([0, 1, 0,  ..., 0, 1, 0])\n",
            "Accuracy of the network on the train set: 0.562293 total: 4222\n",
            "tensor([1, 0, 1,  ..., 1, 1, 0])\n",
            "tensor([1, 1, 0,  ..., 0, 1, 0])\n",
            "\n",
            "\n",
            "Accuracy of the network on test set: 0.568182 total: 1056\n",
            "\n",
            "\n",
            "TEST ACCURACY: (600, 'correct')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "522b18fc-1bea-4966-eca2-62e76217346c",
        "id": "HYtEclpXd2Vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "def test_compas_data():\n",
        "  ''' Get the data \t'''\n",
        "  data_type = 1\n",
        "  X, y, x_control = load_compas_data()\n",
        "  sensitive_attrs = x_control.keys()\n",
        "\n",
        "\n",
        "  ''' Split the data into train and test \t'''\n",
        "  train_fold_size = 0.8\n",
        "  \n",
        "  x_train, y_train, x_control_train, x_test, y_test, x_control_test = split_into_train_test(X, y, x_control, train_fold_size)\n",
        "  #print(y_train[:20])\n",
        "  lb = LabelEncoder()\n",
        "  #y_train = one_hot_encode(lb.fit_transform(y_train))\n",
        "  #y_test = np_utils.to_categorical(one_hot_encode(y_test))\n",
        "  y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "  y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
        "  print(x_train.shape, y_test.shape)\n",
        "  print(\"classes: \", lb.classes_, \" turned into \", [0, 1])\n",
        "  score1 = train_test_classifier(x_train, y_train, x_test, y_test)\n",
        "  \n",
        "  print(\"\\n\\nTEST ACCURACY:\", score1)\n",
        "  return y_train, score1\n",
        "\n",
        "\n",
        "def main():\n",
        "  return test_compas_data()\n",
        "\n",
        "y_train = main()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking for file 'compas-scores-two-years3.csv' in the current directory...\n",
            "File found in current directory..\n",
            "\n",
            "Number of people recidivating within two years\n",
            "-1    2795\n",
            " 1    2483\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "Features we will be using for classification are: ['intercept', 'age_18', 'age_19', 'age_20', 'age_21', 'age_22', 'age_23', 'age_24', 'age_25', 'age_26', 'age_27', 'age_28', 'age_29', 'age_30', 'age_31', 'age_32', 'age_33', 'age_34', 'age_35', 'age_36', 'age_37', 'age_38', 'age_39', 'age_40', 'age_41', 'age_42', 'age_43', 'age_44', 'age_45', 'age_46', 'age_47', 'age_48', 'age_49', 'age_50', 'age_51', 'age_52', 'age_53', 'age_54', 'age_55', 'age_56', 'age_57', 'age_58', 'age_59', 'age_60', 'age_61', 'age_62', 'age_63', 'age_64', 'age_65', 'age_66', 'age_67', 'age_68', 'age_69', 'age_70', 'age_71', 'age_72', 'age_73', 'age_74', 'age_75', 'age_77', 'age_78', 'age_79', 'age_80', 'race', 'sex', 'juv_fel_count_0', 'juv_fel_count_1', 'juv_fel_count_2', 'juv_fel_count_3', 'juv_fel_count_4', 'juv_fel_count_5', 'juv_fel_count_6', 'juv_fel_count_8', 'juv_fel_count_10', 'juv_misd_count_0', 'juv_misd_count_1', 'juv_misd_count_2', 'juv_misd_count_3', 'juv_misd_count_4', 'juv_misd_count_5', 'juv_misd_count_6', 'juv_misd_count_8', 'juv_misd_count_12', 'juv_misd_count_13', 'juv_other_count_0', 'juv_other_count_1', 'juv_other_count_2', 'juv_other_count_3', 'juv_other_count_4', 'juv_other_count_5', 'juv_other_count_6', 'juv_other_count_7', 'priors_count', 'c_charge_degree'] \n",
            "\n",
            "(4222, 94) (1056, 2)\n",
            "classes:  [-1  1]  turned into  [0, 1]\n",
            "TRAIN ON:\t cpu \n",
            "\n",
            "Training DATASET SIZE:  4222 \tNUM OF BATCHES:  1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3760.5740, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(1718.7402, grad_fn=<MulBackward0>) SQ loss:  tensor(2129.2515, grad_fn=<SumBackward0>)\n",
            "Epoch:  1 \tLoss:  3847.99169921875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3201.8022, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(38689.7695, grad_fn=<MulBackward0>) SQ loss:  tensor(2200.7029, grad_fn=<SumBackward0>)\n",
            "Epoch:  2 \tLoss:  40890.47265625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3308.7671, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(27993.2852, grad_fn=<MulBackward0>) SQ loss:  tensor(2121.5513, grad_fn=<SumBackward0>)\n",
            "Epoch:  3 \tLoss:  30114.8359375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4375.9434, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(7872.4341, grad_fn=<MulBackward0>) SQ loss:  tensor(2154.5322, grad_fn=<SumBackward0>)\n",
            "Epoch:  4 \tLoss:  10026.966796875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4424.0371, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8353.3711, grad_fn=<MulBackward0>) SQ loss:  tensor(2250.4431, grad_fn=<SumBackward0>)\n",
            "Epoch:  5 \tLoss:  10603.814453125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4456.1353, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8674.3535, grad_fn=<MulBackward0>) SQ loss:  tensor(2346.5586, grad_fn=<SumBackward0>)\n",
            "Epoch:  6 \tLoss:  11020.912109375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4475.8618, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8871.6191, grad_fn=<MulBackward0>) SQ loss:  tensor(2416.1143, grad_fn=<SumBackward0>)\n",
            "Epoch:  7 \tLoss:  11287.7333984375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4486.3525, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8976.5254, grad_fn=<MulBackward0>) SQ loss:  tensor(2453.5801, grad_fn=<SumBackward0>)\n",
            "Epoch:  8 \tLoss:  11430.10546875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4490.0732, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(9013.7324, grad_fn=<MulBackward0>) SQ loss:  tensor(2462.8628, grad_fn=<SumBackward0>)\n",
            "Epoch:  9 \tLoss:  11476.595703125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4488.6304, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8999.3047, grad_fn=<MulBackward0>) SQ loss:  tensor(2449.3428, grad_fn=<SumBackward0>)\n",
            "Epoch:  10 \tLoss:  11448.6474609375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4483.2876, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8945.8770, grad_fn=<MulBackward0>) SQ loss:  tensor(2418.7705, grad_fn=<SumBackward0>)\n",
            "Epoch:  11 \tLoss:  11364.6474609375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4475.0044, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8863.0449, grad_fn=<MulBackward0>) SQ loss:  tensor(2376.9465, grad_fn=<SumBackward0>)\n",
            "Epoch:  12 \tLoss:  11239.9912109375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4464.3774, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8756.7754, grad_fn=<MulBackward0>) SQ loss:  tensor(2329.1177, grad_fn=<SumBackward0>)\n",
            "Epoch:  13 \tLoss:  11085.892578125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4451.7354, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8630.3535, grad_fn=<MulBackward0>) SQ loss:  tensor(2278.8198, grad_fn=<SumBackward0>)\n",
            "Epoch:  14 \tLoss:  10909.173828125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4437.1040, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8484.0410, grad_fn=<MulBackward0>) SQ loss:  tensor(2227.6521, grad_fn=<SumBackward0>)\n",
            "Epoch:  15 \tLoss:  10711.693359375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4420.4165, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8317.1660, grad_fn=<MulBackward0>) SQ loss:  tensor(2177.3528, grad_fn=<SumBackward0>)\n",
            "Epoch:  16 \tLoss:  10494.5185546875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4401.5757, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(8128.7573, grad_fn=<MulBackward0>) SQ loss:  tensor(2130.2964, grad_fn=<SumBackward0>)\n",
            "Epoch:  17 \tLoss:  10259.0537109375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4380.4639, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(7917.6392, grad_fn=<MulBackward0>) SQ loss:  tensor(2089.2119, grad_fn=<SumBackward0>)\n",
            "Epoch:  18 \tLoss:  10006.8515625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4156.9658, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(5682.6587, grad_fn=<MulBackward0>) SQ loss:  tensor(2057.5457, grad_fn=<SumBackward0>)\n",
            "Epoch:  19 \tLoss:  7740.2041015625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3559.5325, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(2916.7480, grad_fn=<MulBackward0>) SQ loss:  tensor(2039.5521, grad_fn=<SumBackward0>)\n",
            "Epoch:  20 \tLoss:  4956.30029296875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3725.1990, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(1364.9902, grad_fn=<MulBackward0>) SQ loss:  tensor(2039.4250, grad_fn=<SumBackward0>)\n",
            "Epoch:  21 \tLoss:  3404.415283203125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3863.4390, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(2747.3901, grad_fn=<MulBackward0>) SQ loss:  tensor(2040.0760, grad_fn=<SumBackward0>)\n",
            "Epoch:  22 \tLoss:  4787.46630859375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3993.0227, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(4043.2275, grad_fn=<MulBackward0>) SQ loss:  tensor(2040.2520, grad_fn=<SumBackward0>)\n",
            "Epoch:  23 \tLoss:  6083.4794921875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4056.8342, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(4681.3428, grad_fn=<MulBackward0>) SQ loss:  tensor(2039.3372, grad_fn=<SumBackward0>)\n",
            "Epoch:  24 \tLoss:  6720.6796875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4072.1462, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(4834.4629, grad_fn=<MulBackward0>) SQ loss:  tensor(2037.1331, grad_fn=<SumBackward0>)\n",
            "Epoch:  25 \tLoss:  6871.595703125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4052.2817, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(4635.8179, grad_fn=<MulBackward0>) SQ loss:  tensor(2033.6415, grad_fn=<SumBackward0>)\n",
            "Epoch:  26 \tLoss:  6669.45947265625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(4000.3005, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(4116.0059, grad_fn=<MulBackward0>) SQ loss:  tensor(2029.0117, grad_fn=<SumBackward0>)\n",
            "Epoch:  27 \tLoss:  6145.017578125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3926.4121, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(3377.1216, grad_fn=<MulBackward0>) SQ loss:  tensor(2023.4891, grad_fn=<SumBackward0>)\n",
            "Epoch:  28 \tLoss:  5400.61083984375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3853.1758, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(2644.7583, grad_fn=<MulBackward0>) SQ loss:  tensor(2017.3341, grad_fn=<SumBackward0>)\n",
            "Epoch:  29 \tLoss:  4662.09228515625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3781.5117, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(1928.1177, grad_fn=<MulBackward0>) SQ loss:  tensor(2010.9026, grad_fn=<SumBackward0>)\n",
            "Epoch:  30 \tLoss:  3939.020263671875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3717.7947, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(1290.9473, grad_fn=<MulBackward0>) SQ loss:  tensor(2004.5608, grad_fn=<SumBackward0>)\n",
            "Epoch:  31 \tLoss:  3295.508056640625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3664.3286, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(756.2866, grad_fn=<MulBackward0>) SQ loss:  tensor(1998.7113, grad_fn=<SumBackward0>)\n",
            "Epoch:  32 \tLoss:  2754.998046875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3610.0415, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(213.4155, grad_fn=<MulBackward0>) SQ loss:  tensor(1993.7555, grad_fn=<SumBackward0>)\n",
            "Epoch:  33 \tLoss:  2207.1708984375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3564.2158, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(2448.4131, grad_fn=<MulBackward0>) SQ loss:  tensor(1990.0847, grad_fn=<SumBackward0>)\n",
            "Epoch:  34 \tLoss:  4438.498046875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3603.5376, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(148.3765, grad_fn=<MulBackward0>) SQ loss:  tensor(1988.8016, grad_fn=<SumBackward0>)\n",
            "Epoch:  35 \tLoss:  2137.17822265625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3636.8088, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(481.0889, grad_fn=<MulBackward0>) SQ loss:  tensor(1987.9125, grad_fn=<SumBackward0>)\n",
            "Epoch:  36 \tLoss:  2469.00146484375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3660.9382, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(722.3828, grad_fn=<MulBackward0>) SQ loss:  tensor(1986.8892, grad_fn=<SumBackward0>)\n",
            "Epoch:  37 \tLoss:  2709.27197265625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3671.1416, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(824.4165, grad_fn=<MulBackward0>) SQ loss:  tensor(1985.4908, grad_fn=<SumBackward0>)\n",
            "Epoch:  38 \tLoss:  2809.9072265625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3673.1938, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(844.9390, grad_fn=<MulBackward0>) SQ loss:  tensor(1983.6392, grad_fn=<SumBackward0>)\n",
            "Epoch:  39 \tLoss:  2828.578125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3665.7053, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(770.0537, grad_fn=<MulBackward0>) SQ loss:  tensor(1981.3735, grad_fn=<SumBackward0>)\n",
            "Epoch:  40 \tLoss:  2751.42724609375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3649.9912, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(612.9126, grad_fn=<MulBackward0>) SQ loss:  tensor(1978.8068, grad_fn=<SumBackward0>)\n",
            "Epoch:  41 \tLoss:  2591.71923828125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3634.6838, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(459.8389, grad_fn=<MulBackward0>) SQ loss:  tensor(1976.1445, grad_fn=<SumBackward0>)\n",
            "Epoch:  42 \tLoss:  2435.9833984375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3608.4373, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(197.3730, grad_fn=<MulBackward0>) SQ loss:  tensor(1973.6118, grad_fn=<SumBackward0>)\n",
            "Epoch:  43 \tLoss:  2170.98486328125\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3583.0151, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(568.4814, grad_fn=<MulBackward0>) SQ loss:  tensor(1971.4725, grad_fn=<SumBackward0>)\n",
            "Epoch:  44 \tLoss:  2539.9541015625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3628.9734, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(402.7344, grad_fn=<MulBackward0>) SQ loss:  tensor(1970.4684, grad_fn=<SumBackward0>)\n",
            "Epoch:  45 \tLoss:  2373.20263671875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3658.7346, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(700.3467, grad_fn=<MulBackward0>) SQ loss:  tensor(1970.0447, grad_fn=<SumBackward0>)\n",
            "Epoch:  46 \tLoss:  2670.391357421875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3685.1362, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(964.3628, grad_fn=<MulBackward0>) SQ loss:  tensor(1969.6643, grad_fn=<SumBackward0>)\n",
            "Epoch:  47 \tLoss:  2934.027099609375\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3698.1304, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(1094.3042, grad_fn=<MulBackward0>) SQ loss:  tensor(1969.0031, grad_fn=<SumBackward0>)\n",
            "Epoch:  48 \tLoss:  3063.30712890625\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3705.9365, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(1172.3657, grad_fn=<MulBackward0>) SQ loss:  tensor(1967.9185, grad_fn=<SumBackward0>)\n",
            "Epoch:  49 \tLoss:  3140.2841796875\n",
            "\n",
            "\n",
            "\t\tUTILITY:  tensor(3703.7930, grad_fn=<SumBackward0>) \tTAU*n:  3588.7  Regularizer loss:  tensor(1150.9302, grad_fn=<MulBackward0>) SQ loss:  tensor(1966.3948, grad_fn=<SumBackward0>)\n",
            "Epoch:  50 \tLoss:  3117.324951171875\n",
            "tensor([1, 1, 1,  ..., 0, 0, 0])\n",
            "tensor([0, 1, 0,  ..., 0, 1, 0])\n",
            "Accuracy of the network on the train set: 0.646376 total: 4222\n",
            "tensor([1, 0, 1,  ..., 0, 1, 0])\n",
            "tensor([1, 1, 0,  ..., 0, 1, 0])\n",
            "\n",
            "\n",
            "Accuracy of the network on test set: 0.655303 total: 1056\n",
            "\n",
            "\n",
            "TEST ACCURACY: (692, 'correct')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}